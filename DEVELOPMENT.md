# Instrukcja deweloperska: Unitree G1 Charging Mission

Ten dokument opisuje proces konfiguracji rodowiska, budowania kodu oraz uruchamiania stacku technologicznego dla robota Unitree G1 EDU. Projekt wykorzystuje **ROS 2 (Humble/Jazzy)** i architektur moduow opart na **Action Servers**.

## 1. Wymagania wstpne (Prerequisites)

Przed rozpoczciem pracy upewnij si, 偶e Tw贸j system spenia poni偶sze wymagania:

*   **OS:** Ubuntu 22.04 LTS (Jammy Jellyfish).
*   **ROS 2 Distro:** Humble Hawksbill (lub Jazzy Jalisco).
*   **Unitree SDK 2:** Zainstalowane i skonfigurowane w systemie (wymagane do komunikacji z `low_level_interface`).
*   **Python:** Wersja 3.10+.
*   **Build System:** `colcon`.

## 2. Konfiguracja workspace

Wykonaj poni偶sze kroki w terminalu, aby przygotowa przestrze robocz.

### Krok 1: Klonowanie repozytorium
Utw贸rz katalog roboczy (jeli jeszcze go nie masz) i sklonuj kod:

```bash
mkdir -p ~/ros2_ws/src
cd ~/ros2_ws/src
git clone https://github.com/AI-robot-lab/unitree-g1-auto-charger-plugging.git
```

### Krok 2: Instalacja zale偶noci
U偶yj narzdzia `rosdep`, aby automatycznie zainstalowa brakujce biblioteki systemowe zdefiniowane w plikach `package.xml`:

```bash
cd ~/ros2_ws
sudo apt update
rosdep update
rosdep install --from-paths src --ignore-src -y
```

### Krok 3: Budowanie projektu (Build)
Skompiluj wszystkie pakiety u偶ywajc `colcon`. Flaga `--symlink-install` pozwala na edycj skrypt贸w w Pythonie bez koniecznoci ponownej kompilacji.

```bash
colcon build --symlink-install
```

Jeli kompilacja zakoczy si sukcesem, zobaczysz komunikat: `Summary: X packages finished`.

### Krok 4: Source environment
Aby system widzia Twoje nowe pakiety, musisz wykona *source* pliku instalacyjnego:

```bash
source install/setup.bash
```
*(Zalecamy dodanie tej komendy do pliku `~/.bashrc`)*.

---

## 3. Architektura i pakiety (Packages Overview)

Projekt jest podzielony na niezale偶ne pakiety. Poni偶ej znajduje si opis ich odpowiedzialnoci oraz g贸wne `nodes`.

###  `mission_control` (M贸zg/ logika systemu)
Zawiera g贸wn maszyn stan贸w (**State Machine**), kt贸ra zarzdza przebiegiem misji.
*   **Main Node:** `state_machine_action_client.py`
*   **Funkcja:** Dziaa jako **Action Client**. Wysya cele (Goals) do innych podsystem贸w.
*   **Workflow:** `NAV_TO_STATION` $\to$ `DETECT_HANDLE` $\to$ `MANIPULATE_GRASP` $\to$ `NAV_TO_CAR`.

###  `perception` (Percepcja/ wizja komputerowa)
Odpowiada za przetwarzanie obrazu z kamer RealSense/Unitree.
*   **Main Node:** `perception_action_server.py`
*   **Action Server:** Obsuguje `Detect.action`.
*   **Input:** `/camera/color/image_raw`
*   **Output:** Pozycja uchwytu `geometry_msgs/Pose`.

###  `navigation` (Nawigacja/ przemieszczanie si)
Odpowiada za przemieszczanie si robota po laboratorium.
*   **Main Node:** `navigation_action_server.py`
*   **Action Server:** Obsuguje `Navigate.action`.
*   **Integracja:** Wykorzystuje Unitree SDK do wysyania komend prdkoci (`cmd_vel`).

###  `manipulation` (Manipulacja/ ruchy ramion)
Odpowiada za planowanie ruchu rk i chwytanie.
*   **Main Node:** `manipulation_action_server.py`
*   **Action Server:** Obsuguje `Manipulate.action`.
*   **Logika:** Oblicza IK (Inverse Kinematics) dla zadania `grasp_handle` lub `insert_plug`.

###  `charging_interfaces`
Pakiet zawierajcy wycznie definicje wiadomoci `.msg` i akcji `.action`.
*   `Navigate.action`
*   `Manipulate.action`
*   `Detect.action`

---

## 4. Uruchamianie (Running the Project)

### Tryb symulacji (Mock Mode)
Mo偶esz uruchomi system bez fizycznego robota. W tym trybie wzy `navigation` i `manipulation` symuluj dziaanie (czekaj 2 sekundy i zwracaj `success`).

1. Uruchom serwery akcji oraz klienta misji w osobnych terminalach:
   ```bash
   ros2 run navigation navigation_action_server
   ros2 run manipulation manipulation_action_server
   ros2 run perception perception_action_server
   ros2 run mission_control state_machine_action_client
   ```

2. Powiniene zobaczy w logach konsoli sekwencj:
   ```text
   [INFO] [state_machine_action_client]: State: NAV_TO_STATION
   [INFO] [navigation_action_server]: Received navigation goal to pose: [2.0, 0.0, 0.0]
   [INFO] [navigation_action_server]: Navigation goal succeeded
   [INFO] [state_machine_action_client]: State: DETECT_HANDLE
   ```

### Tryb rzeczywisty (Real Robot)
**UWAGA:** Wymaga podczenia do sieci robota i trzymania E-Stop w rku.

1. Upewnij si, 偶e masz poczenie z robotem (ping `192.168.123.xx`).
2. Uruchom sterowniki sprztowe:
   ```bash
   ros2 launch g1_hardware hardware_interface.launch.py
   ```
3. W osobnym terminalu uruchom logik misji:
   ```bash
   ros2 run mission_control state_machine_action_client
   ```

---

## 5. Workflow pracy z kodem

### Dodawanie nowej funkcjonalnoci
1. Utw贸rz now ga藕 (`feature branch`):
   ```bash
   git checkout -b feature/improved-vision
   ```
2. Wprowad藕 zmiany w kodzie (np. w `perception/src/yolo_detector.cpp`).
3. Przed wysaniem zmian sformatuj kod (zgodnie z `ament_lint`):
   ```bash
   colcon test --packages-select perception
   ```
4. Wylij zmiany (Push) i utw贸rz **Pull Request** na GitHubie.

### Debugowanie
U偶yj narzdzia `rqt_graph`, aby zweryfikowa poczenia midzy wzami i akcjami:
```bash
rqt_graph
```
Powiniene widzie `state_machine_action_client` poczony liniami akcji z wzami `perception`, `navigation` i `manipulation`.
